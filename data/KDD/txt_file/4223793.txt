generalized component analysis for text with heterogeneous attributes we present a class of richly structured , undirected hidden variable models suitable for simultaneously modeling text along with other attributes encoded in different modalities . our model generalizes techniques such as principal component analysis to heterogeneous data types . in contrast to other approaches , this framework allows modalities such as words , authors and timestamps to be captured in their natural , probabilistic encodings . a latent space representation for a previously unseen document can be obtained through a fast matrix multiplication using our method . we demonstrate the effectiveness of our framework on the task of author prediction from 13 years of the nips conference proceedings and for a recipient prediction task using a 10-month academic email archive of a researcher . our approach should be more broadly applicable to many real-world applications where one wishes to efficiently make predictions for a large number of potential outputs using dimensionality reduction in a well defined probabilistic framework .