is question answering an acquired skill ? we present a question answering ( qa ) system which learns how to detect and rank answer passages by analyzing questions and their answers ( qa pairs ) provided as training data . we built our system in only a few person-months using off-the-shelf components : a part-of-speech tagger , a shallow parser , a lexical network , and a few well-known supervised learning algorithms . in contrast , many of the top trec qa systems are large group efforts , using customized ontologies , question classifiers , and highly tuned ranking functions . our ease of deployment arises from using generic , trainable algorithms that exploit simple feature extractors on qa pairs . with trec qa data , our system achieves mean reciprocal rank ( mrr ) that compares favorably with the best scores in recent years , and generalizes from one corpus to another . our key technique is to recover , from the question , fragments of what might have been posed as a structured query , had a suitable schema been available . comprises selectors : tokens that are likely to appear ( almost ) unchanged in an answer passage . the other fragment contains question tokens which give clues about the answer type , and are expected to be replaced in the answer passage by tokens which specialize or instantiate the desired answer type . selectors are like constants in where-clauses in relational queries , and answer types are like column names . we present new algorithms for locating selectors and answer type clues and using them in scoring passages with respect to a question . 