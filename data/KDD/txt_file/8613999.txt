predicting rare classes : can boosting make any weak learner strong ? boosting is a strong ensemble-based learning algorithm with the promise of iteratively improving the classification accuracy using any base learner , as long as it satisfies the condition of yielding weighted accuracy ) 0.5 . in this paper , we analyze boosting with respect to this basic condition on the base learner , to see if boosting ensures prediction of rarely occurring events with high recall and precision . first we show that a base learner can satisfy the required condition even for poor recall or precision levels , especially for very rare classes . furthermore , we show that the intelligent weight updating mechanism in boosting , even in its strong cost-sensitive form , does not prevent cases where the base learner always achieves high precision but poor recall or high recall but poor precision , when mapped to the original distribution . in either of these cases , we show that the voting mechanism of boosting falls to achieve good overall recall and precision for the ensemble . in effect , our analysis indicates that one can not be blind to the base learner performance , and just rely on the boosting mechanism to take care of its weakness . we validate our arguments empirically on variety of real and synthetic rare class problems . in particular , using adacost as the boosting algorithm , and variations of pnrule and ripper as the base learners , we show that if algorithm a achieves better recall-precision balance than algorithm b , then using a as the base learner in adacost yields significantly better performance than using b as the base learner .