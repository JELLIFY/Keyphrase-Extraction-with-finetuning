fast collapsed gibbs sampling for latent dirichlet allocation in this paper we introduce a novel collapsed gibbs sampling method for the widely used latent dirichlet allocation ( lda ) model . our new method results in significant speedups on real world text corpora . conventional gibbs sampling schemes for lda require o ( k ) operations per sample where k is the number of topics in the model . our proposed method draws equivalent samples but requires on average significantly less then k operations per sample . on real-word corpora fastlda can be as much as 8 times faster than the standard collapsed gibbs sampler for lda . no approximations are necessary , and we show that our fast sampling scheme produces exactly the same results as the standard ( but slower ) sampling scheme . experiments on four real world data sets demonstrate speedups for a wide range of collection sizes . for the pubmed collection of over 8 million documents with a required computation time of 6 cpu months for lda , our speedup of 5.7 can save 5 cpu months of computation .