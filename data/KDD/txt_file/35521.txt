cross-training : learning probabilistic mappings between topics classification is a well-established operation in text mining . given a set of labels a and a set da of training documents tagged with these labels , a classifier learns to assign labels to unlabeled test documents . suppose we also had available a different set of labels b , together with a set of documents db marked with labels from b. if a and b have some semantic overlap , can the availability of db help us build a better classifier for a , and vice versa ? we answer this question in the affirmative by proposing cross-training : a new approach to semi-supervised learning in presence of multiple label sets . we give distributional and discriminative algorithms for cross-training and show , through extensive experiments , that cross-training can discover and exploit probabilistic relations between two taxonomies for more accurate classification .