large-scale sparse logistic regression logistic regression is a well-known classification method that has been used widely in many applications of data mining , machine learning , computer vision , and bioinformatics . sparse logistic regression embeds feature selection in the classification framework using the l1-norm regularization , and is attractive in many applications involving high-dimensional data . in this paper , we propose lassplore for solving large-scale sparse logistic regression . specifically , we formulate the problem as the l1-ball constrained smooth convex optimization , and propose to solve the problem using the nesterov 's method , an optimal first-order black-box method for smooth convex optimization . one of the critical issues in the use of the nesterov 's method is the estimation of the step size at each of the optimization iterations . previous approaches either applies the constant step size which assumes that the lipschitz gradient is known in advance , or requires a sequence of decreasing step size which leads to slow convergence in practice . in this paper , we propose an adaptive line search scheme which allows to tune the step size adaptively and meanwhile guarantees the optimal convergence rate . empirical comparisons with several state-of-the-art algorithms demonstrate the efficiency of the proposed lassplore algorithm for large-scale problems .