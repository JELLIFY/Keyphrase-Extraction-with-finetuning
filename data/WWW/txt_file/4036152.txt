the discoverability of the web previous studies have highlighted the high arrival rate of new contenton the web . we study the extent to which this new content can beefficiently discovered by a crawler . our study has two parts . first , we study the inherent difficulty of the discovery problem using amaximum cover formulation , under an assumption of perfect estimates oflikely sources of links to new content . second , we relax thisassumption and study a more realistic setting in which algorithms mustuse historical statistics to estimate which pages are most likely toyield links to new content . we recommend a simple algorithm thatperforms comparably to all approaches we consider . we measure the emphoverhead of discovering new content , defined asthe average number of fetches required to discover one new page . weshow first that with perfect foreknowledge of where to explore forlinks to new content , it is possible to discover 90 % of all newcontent with under 3 % overhead , and 100 % of new content with 9 % overhead . but actual algorithms , which do not have access to perfectforeknowledge , face a more difficult task : one quarter of new contentis simply not amenable to efficient discovery . of the remaining threequarters , 80 % of new content during a given week may be discoveredwith 160 % overhead if content is recrawled fully on a monthly basis . 