smoothing techniques for adaptive online language models : topic tracking in tweet streams we are interested in the problem of tracking broad topics such as `` baseball '' and `` fashion '' in continuous streams of short texts , exemplified by tweets from the microblogging service twitter . the task is conceived as a language modeling problem where per-topic models are trained using hashtags in the tweet stream , which serve as proxies for topic labels . simple perplexity-based classifiers are then applied to filter the tweet stream for topics of interest . within this framework , we evaluate , both intrinsically and extrinsically , smoothing techniques for integrating `` foreground '' models ( to capture recency ) and `` background '' models ( to combat sparsity ) , as well as different techniques for retaining history . experiments show that unigram language models smoothed using a normalized extension of stupid backoff and a simple queue for history retention performs well on the task .