a word at a time : computing word relatedness using temporal semantic analysis computing the degree of semantic relatedness of words is a key functionality of many language applications such as search , clustering , and disambiguation . previous approaches to computing semantic relatedness mostly used static language resources , while essentially ignoring their temporal aspects . we believe that a considerable amount of relatedness information can also be found in studying patterns of word usage over time . consider , for instance , a newspaper archive spanning many years . two words such as `` war '' and `` peace '' might rarely co-occur in the same articles , yet their patterns of use over time might be similar . in this paper , we propose a new semantic relatedness model , temporal semantic analysis ( tsa ) , which captures this temporal information . the previous state of the art method , explicit semantic analysis ( esa ) , represented word semantics as a vector of concepts . tsa uses a more refined representation , where each concept is no longer scalar , but is instead represented as time series over a corpus of temporally-ordered documents . to the best of our knowledge , this is the first attempt to incorporate temporal evidence into models of semantic relatedness . empirical evaluation shows that tsa provides consistent improvements over the state of the art esa results on multiple benchmarks . 