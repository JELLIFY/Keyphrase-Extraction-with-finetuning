primal sparse max-margin markov networks max-margin markov networks ( m3n ) have shown great promise in structured prediction and relational learning . due to the kkt conditions , the m3n enjoys dual sparsity . however , the existing m3n formulation does not enjoy primal sparsity , which is a desirable property for selecting significant features and reducing the risk of over-fitting . in this paper , we present an l1-norm regularized max-margin markov network ( l1-m3n ) , which enjoys dual and primal sparsity simultaneously . to learn an l1-m3n , we present three methods including projected sub-gradient , cutting-plane , and a novel em-style algorithm , which is based on an equivalence between l1-m3n and an adaptive m3n . we perform extensive empirical studies on both synthetic and real data sets . our experimental results show that : ( 1 ) l1-m3n can effectively select significant features ; ( 2 ) l1-m3n can perform as well as the pseudo-primal sparse laplace m3n in prediction accuracy , while consistently outperforms other competing methods that enjoy either primal or dual sparsity ; and ( 3 ) the em-algorithm is more robust than the other two in pre-diction accuracy and time efficiency .