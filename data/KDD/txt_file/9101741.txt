learning classifiers from only positive and unlabeled data the input to an algorithm that learns a binary classifier normally consists of two sets of examples , where one set consists of positive examples of the concept to be learned , and the other set consists of negative examples . however , it is often the case that the available training data are an incomplete set of positive examples , and a set of unlabeled examples , some of which are positive and some of which are negative . the problem solved in this paper is how to learn a standard binary classifier given a nontraditional training set of this nature . under the assumption that the labeled examples are selected randomly from the positive examples , we show that a classifier trained on positive and unlabeled examples predicts probabilities that differ by only a constant factor from the true conditional probabilities of being positive . we show how to use this result in two different ways to learn a classifier from a nontraditional training set . we then apply these two new methods to solve a real-world problem : identifying protein records that should be included in an incomplete specialized molecular biology database . our experiments in this domain show that models trained using the new methods perform better than the current state-of-the-art biased svm method for learning from positive and unlabeled examples .