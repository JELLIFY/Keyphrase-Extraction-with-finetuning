collective context-aware topic models for entity disambiguation a crucial step in adding structure to unstructured data is to identify references to entities and disambiguate them . such disambiguated references can help enhance readability and draw similarities across different pieces of running text in an automated fashion . previous research has tackled this problem by first forming a catalog of entities from a knowledge base , such as wikipedia , and then using this catalog to disambiguate references in unseen text . however , most of the previously proposed models either do not use all text in the knowledge base , potentially missing out on discriminative features , or do not exploit word-entity proximity to learn high-quality catalogs . in this work , we propose topic models that keep track of the context of every word in the knowledge base ; so that words appearing within the same context as an entity are more likely to be associated with that entity . thus , our topic models utilize all text present in the knowledge base and help learn high-quality catalogs . our models also learn groups of co-occurring entities thus enabling collective disambiguation . unlike most previous topic models , our models are non-parametric and do not require the user to specify the exact number of groups present in the knowledge base . in experiments performed on an extract of wikipedia containing almost 60,000 references , our models outperform svm-based baselines by as much as 18 % in terms of disambiguation accuracy translating to an increment of almost 11,000 correctly disambiguated references . 