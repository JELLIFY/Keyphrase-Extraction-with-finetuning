a distributed learning framework for heterogeneous data sources we present a probabilistic model-based framework for distributed learning that takes into account privacy restrictions and is applicable to scenarios where the different sites have diverse , possibly overlapping subsets of features . our framework decouples data privacy issues from knowledge integration issues by requiring the individual sites to share only privacy-safe probabilistic models of the local data , which are then integrated to obtain a global probabilistic model based on the union of the features available at all the sites . we provide a mathematical formulation of the model integration problem using the maximum likelihood and maximum entropy principles and describe iterative algorithms that are guaranteed to converge to the optimal solution . for certain commonly occurring special cases involving hierarchically ordered feature sets or conditional independence , we obtain closed form solutions and use these to propose an efficient alternative scheme by recursive decomposition of the model integration problem . to address interpretability concerns , we also present a modified formulation where the global model is assumed to belong to a specified parametric family . finally , to highlight the generality of our framework , we provide empirical results for various learning tasks such as clustering and classification on different kinds of datasets consisting of continuous vector , categorical and directional attributes . the results show that high quality global models can be obtained without much loss of privacy .