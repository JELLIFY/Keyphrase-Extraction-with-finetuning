data mining in metric space : an empirical analysis of supervised learning performance criteria many criteria can be used to evaluate the performance of supervised learning . different criteria are appropriate in different settings , and it is not always clear which criteria to use . a further complication is that learning methods that perform well on one criterion may not perform well on other criteria . for example , svms and boosting are designed to optimize accuracy , whereas neural nets typically optimize squared error or cross entropy . we conducted an empirical study using a variety of learning methods ( svms , neural nets , k-nearest neighbor , bagged and boosted trees , and boosted stumps ) to compare nine boolean classification performance metrics : accuracy , lift , f-score , area under the roc curve , average precision , precision\/recall break-even point , squared error , cross entropy , and probability calibration . multidimensional scaling ( mds ) shows that these metrics span a low dimensional manifold . the three metrics that are appropriate when predictions are interpreted as probabilities : squared error , cross entropy , and calibration , lay in one part of metric space far away from metrics that depend on the relative order of the predicted values : roc area , average precision , break-even point , and lift . in between them fall two metrics that depend on comparing predictions to a threshold : accuracy and f-score . as expected , maximum margin methods such as svms and boosted trees have excellent performance on metrics like accuracy , but perform poorly on probability metrics such as squared error . what was not expected was that the margin methods have excellent performance on ordering metrics such as roc area and average precision . we introduce a new metric , sar , that combines squared error , accuracy , and roc area into one metric . mds and correlation analysis shows that sar is centrally located and correlates well with other metrics , suggesting that it is a good general purpose metric to use when more specific criteria are not known .