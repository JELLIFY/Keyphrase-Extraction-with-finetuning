a scalable two-stage approach for a class of dimensionality reduction techniques dimensionality reduction plays an important role in many data mining applications involving high-dimensional data . many existing dimensionality reduction techniques can be formulated as a generalized eigenvalue problem , which does not scale to large-size problems . prior work transforms the generalized eigenvalue problem into an equivalent least squares formulation , which can then be solved efficiently . however , the equivalence relationship only holds under certain assumptions without regularization , which severely limits their applicability in practice . in this paper , an efficient two-stage approach is proposed to solve a class of dimensionality reduction techniques , including canonical correlation analysis , orthonormal partial least squares , linear discriminant analysis , and hypergraph spectral learning . the proposed two-stage approach scales linearly in terms of both the sample size and data dimensionality . the main contributions of this paper include ( 1 ) we rigorously establish the equivalence relationship between the proposed two-stage approach and the original formulation without any assumption ; and ( 2 ) we show that the equivalence relationship still holds in the regularization setting . we have conducted extensive experiments using both synthetic and real-world data sets . our experimental results confirm the equivalence relationship established in this paper . results also demonstrate the scalability of the proposed two-stage approach .