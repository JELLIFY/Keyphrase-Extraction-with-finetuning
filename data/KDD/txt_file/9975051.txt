differentially private recommender systems : building privacy into the net we consider the problem of producing recommendations from collective user behavior while simultaneously providing guarantees of privacy for these users . specifically , we consider the netflix prize data set , and its leading algorithms , adapted to the framework of differential privacy . unlike prior privacy work concerned with cryptographically securing the computation of recommendations , differential privacy constrains a computation in a way that precludes any inference about the underlying records from its output . such algorithms necessarily introduce uncertainty -- i.e. , noise -- to computations , trading accuracy for privacy . we find that several of the leading approaches in the netflix prize competition can be adapted to provide differential privacy , without significantly degrading their accuracy . to adapt these algorithms , we explicitly factor them into two parts , an aggregation\/learning phase that can be performed with differential privacy guarantees , and an individual recommendation phase that uses the learned correlations and an individual 's data to provide personalized recommendations . the adaptations are non-trivial , and involve both careful analysis of the per-record sensitivity of the algorithms to calibrate noise , as well as new post-processing steps to mitigate the impact of this noise . we measure the empirical trade-off between accuracy and privacy in these adaptations , and find that we can provide non-trivial formal privacy guarantees while still outperforming the cinematch baseline netflix provides .