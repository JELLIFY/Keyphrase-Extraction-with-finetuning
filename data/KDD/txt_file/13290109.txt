discriminative topic modeling based on manifold learning topic modeling has been popularly used for data analysis in various domains including text documents . previous topic models , such as probabilistic latent semantic analysis ( plsa ) and latent dirichlet allocation ( lda ) , have shown impressive success in discovering low-rank hidden structures for modeling text documents . these models , however , do not take into account the manifold structure of data , which is generally informative for the non-linear dimensionality reduction mapping . more recent models , namely laplacian plsi ( lapplsi ) and locally-consistent topic model ( ltm ) , have incorporated the local manifold structure into topic models and have shown the resulting benefits . but these approaches fall short of the full discriminating power of manifold learning as they only enhance the proximity between the low-rank representations of neighboring pairs without any consideration for non-neighboring pairs . in this paper , we propose discriminative topic model ( dtm ) that separates non-neighboring pairs from each other in addition to bringing neighboring pairs closer together , thereby preserving the global manifold structure as well as improving the local consistency . we also present a novel model fitting algorithm based on the generalized em and the concept of pareto improvement . as a result , dtm achieves higher classification performance in a semi-supervised setting by effectively exposing the manifold structure of data . we provide empirical evidence on text corpora to demonstrate the success of dtm in terms of classification accuracy and robustness to parameters compared to state-of-the-art techniques .