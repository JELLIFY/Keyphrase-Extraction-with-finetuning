training structural svms with kernels using sampled cuts discriminative training for structured outputs has found increasing applications in areas such as natural language processing , bioinformatics , information retrieval , and computer vision . focusing on large-margin methods , the most general ( in terms of loss function and model structure ) training algorithms known to date are based on cutting-plane approaches . while these algorithms are very efficient for linear models , their training complexity becomes quadratic in the number of examples when kernels are used . to overcome this bottleneck , we propose new training algorithms that use approximate cutting planes and random sampling to enable efficient training with kernels . we prove that these algorithms have improved time complexity while providing approximation guarantees . in empirical evaluations , our algorithms produced solutions with training and test error rates close to those of exact solvers . even on binary classification problems where highly optimized conventional training methods exist ( e.g. svm-light ) , our methods are about an order of magnitude faster than conventional training methods on large datasets , while remaining competitive in speed on datasets of medium size .