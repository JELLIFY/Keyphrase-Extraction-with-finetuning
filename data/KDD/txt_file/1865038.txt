the ioc algorithm : efficient many-class non-parametric classification for high-dimensional data this paper is about a variant of k nearest neighbor classification on large many-class high dimensional datasets . k nearest neighbor remains a popular classification technique , especially in areas such as computer vision , drug activity prediction and astrophysics . furthermore , many more modern classifiers , such as kernel-based bayes classifiers or the prediction phase of svms , require computational regimes similar to k-nn . we believe that tractable k-nn algorithms therefore continue to be important . this paper relies on the insight that even with many classes , the task of finding the majority class among the k nearest neighbors of a query need not require us to explicitly find those k nearest neighbors . this insight was previously used in ( liu et al. , 2003 ) in two algorithms called kns2 and kns3 which dealt with fast classification in the case of two classes . in this paper we show how a different approach , ioc ( standing for the international olympic committee ) can apply to the case of n classes where n ) 2 . ioc assumes a slightly different processing of the datapoints in the neighborhood of the query . this allows it to search a set of metric trees , one for each class . during the searches it is possible to quickly prune away classes that can not possibly be the majority . we give experimental results on datasets of up to 5.8 x 105 records and 1.5 x 103 attributes , frequently showing an order of magnitude acceleration compared with each of ( i ) conventional linear scan , ( ii ) a well-known independent sr-tree implementation of conventional k-nn and ( iii ) a highly optimized conventional k-nn metric tree search .