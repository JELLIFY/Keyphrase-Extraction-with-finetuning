a unified approach to learning task-specific bit vector representations for fast nearest neighbor search fast nearest neighbor search is necessary for a variety of large scale web applications such as information retrieval , nearest neighbor classification and nearest neighbor regression . recently a number of machine learning algorithms have been proposed for representing the data to be searched as ( short ) bit vectors and then using hashing to do rapid search . these algorithms have been limited in their applicability in that they are suited for only one type of task -- e.g. spectral hashing learns bit vector representations for retrieval , but not say , classification . in this paper we present a unified approach to learning bit vector representations for many applications that use nearest neighbor search . the main contribution is a single learning algorithm that can be customized to learn a bit vector representation suited for the task at hand . this broadens the usefulness of bit vector representations to tasks beyond just conventional retrieval . we propose a learning-to-rank formulation to learn the bit vector representation of the data . lambdarank algorithm is used for learning a function that computes a task-specific bit vector from an input data vector . our approach outperforms state-of-the-art nearest neighbor methods on a number of real world text and image classification and retrieval datasets . it is scalable and learns a 32-bit representation on 1.46 million training cases in two days . 