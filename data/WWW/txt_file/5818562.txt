efficient url caching for world wide web crawling crawling the web is deceptively simple : the basic algorithm is ( a ) fetch a page ( b ) parse it to extract all linked urls ( c ) for all the urls not seen before , repeat ( a ) - ( c ) . however , the size of the web ( estimated at over 4 billion pages ) and its rate of change ( estimated at 7 % per week ) move this plan from a trivial programming exercise to a serious algorithmic and system design challenge . indeed , these two factors alone imply that for a reasonably fresh and complete crawl of the web , step ( a ) must be executed about a thousand times per second , and thus the membership test ( c ) must be done well over ten thousand times per second against a set too large to store in main memory . this requires a distributed architecture , which further complicates the membership test . a crucial way to speed up the test is to cache , that is , to store in main memory a ( dynamic ) subset of the `` seen '' urls . the main goal of this paper is to carefully investigate several url caching techniques for web crawling . we consider both practical algorithms : random replacement , static cache , lru , and clock , and theoretical limits : clairvoyant caching and infinite cache . we performed about 1,800 simulations using these algorithms with various cache sizes , using actual log data extracted from a massive 33 day web crawl that issued over one billion http requests . our main conclusion is that caching is very effective - in our setup , a cache of roughly 50,000 entries can achieve a hit rate of almost 80 % . interestingly , this cache size falls at a critical point : a substantially smaller cache is much less effective while a substantially larger cache brings little additional benefit . we conjecture that such critical points are inherent to our problem and venture an explanation for this phenomenon . 