sitemaps : above and beyond the crawl of duty comprehensive coverage of the public web is crucial to web search engines . search engines use crawlers to retrieve pages and then discover new ones by extracting the pages ' outgoing links . however , the set of pages reachable from the publicly linked web is estimated to be significantly smaller than the invisible web , the set of documents that have no incoming links and can only be retrieved through web applications and web forms . the sitemaps protocol is a fast-growing web protocol supported jointly by major search engines to help content creators and search engines unlock this hidden data by making it available to search engines . in this paper , we perform a detailed study of how `` classic '' discovery crawling compares with sitemaps , in key measures such as coverage and freshness over key representative websites as well as over billions of urls seen at google . we observe that sitemaps and discovery crawling complement each other very well , and offer different tradeoffs . 