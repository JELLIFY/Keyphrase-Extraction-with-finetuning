structured learning for non-smooth ranking losses learning to rank from relevance judgment is an active research area . itemwise score regression , pairwise preference satisfaction , and listwise structured learning are the major techniques in use . listwise structured learning has been applied recently to optimize important non-decomposable ranking criteria like auc ( area under roc curve ) and map ( mean average precision ) . we propose new , almost-linear-time algorithms to optimize for two other criteria widely used to evaluate search systems : mrr ( mean reciprocal rank ) and ndcg ( normalized discounted cumulative gain ) in the max-margin structured learning framework . we also demonstrate that , for different ranking criteria , one may need to use different feature maps . search applications should not be optimized in favor of a single criterion , because they need to cater to a variety of queries . e.g. , mrr is best for navigational queries , while ndcg is best for informational queries . a key contribution of this paper is to fold multiple ranking loss functions into a multi-criteria max-margin optimization . the result is a single , robust ranking model that is close to the best accuracy of learners trained on individual criteria . in fact , experiments over the popular letor and trec data sets show that , contrary to conventional wisdom , a test criterion is often not best served by training with the same individual criterion .