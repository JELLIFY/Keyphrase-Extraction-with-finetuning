issues in evaluation of stream learning algorithms learning from data streams is a research area of increasing importance . nowadays , several stream learning algorithms have been developed . most of them learn decision models that continuously evolve over time , run in resource-aware environments , detect and react to changes in the environment generating data . one important issue , not yet conveniently addressed , is the design of experimental work to evaluate and compare decision models that evolve over time . there are no golden standards for assessing performance in non-stationary environments . this paper proposes a general framework for assessing predictive stream learning algorithms . we defend the use of predictive sequential methods for error estimate - the prequential error . the prequential error allows us to monitor the evolution of the performance of models that evolve over time . nevertheless , it is known to be a pessimistic estimator in comparison to holdout estimates . to obtain more reliable estimators we need some forgetting mechanism . two viable alternatives are : sliding windows and fading factors . we observe that the prequential error converges to an holdout estimator when estimated over a sliding window or using fading factors . we present illustrative examples of the use of prequential error estimators , using fading factors , for the tasks of : i ) assessing performance of a learning algorithm ; ii ) comparing learning algorithms ; iii ) hypothesis testing using mcnemar test ; and iv ) change detection using page-hinkley test . in these tasks , the prequential error estimated using fading factors provide reliable estimators . in comparison to sliding windows , fading factors are faster and memory-less , a requirement for streaming applications . this paper is a contribution to a discussion in the good-practices on performance assessment when learning dynamic models that evolve over time .