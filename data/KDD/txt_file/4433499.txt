efficient handling of high-dimensional feature spaces by randomized classifier ensembles handling massive datasets is a difficult problem not only due to prohibitively large numbers of entries but in some cases also due to the very high dimensionality of the data . often , severe feature selection is performed to limit the number of attributes to a manageable size , which unfortunately can lead to a loss of useful information . feature space reduction may well be necessary for many stand-alone classifiers , but recent advances in the area of ensemble classifier techniques indicate that overall accurate classifier aggregates can be learned even if each individual classifier operates on incomplete `` feature view '' training data , i.e. , such where certain input attributes are excluded . in fact , by using only small random subsets of features to build individual component classifiers , surprisingly accurate and robust models can be created . in this work we demonstrate how these types of architectures effectively reduce the feature space for submodels and groups of sub-models , which lends itself to efficient sequential and\/or parallel implementations . experiments with a randomized version of adaboost are used to support our arguments , using the text classification task as an example .