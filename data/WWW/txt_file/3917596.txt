do not crawl in the dust : different urls with similar text we consider the problem of dust : different urls with similar text . such duplicate urls are prevalent in web sites , as web server software often uses aliases and redirections , translates urls to some canonical form , and dynamically generates the same page from various different url requests . we present a novel algorithm , dustbuster , for uncovering dust ; that is , for discovering rules for transforming a given url to others that are likely to have similar content . dustbuster is able to detect dust effectively from previous crawl logs or web server logs , without examining page contents . verifying these rules via sampling requires fetching few actual web pages . search engines can benefit from this information to increase the effectiveness of crawling , reduce indexing overhead as well as improve the quality of popularity statistics such as pagerank . 