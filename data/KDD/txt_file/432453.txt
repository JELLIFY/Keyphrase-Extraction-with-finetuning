local sparsity control for naive bayes with extreme misclassification costs in applications of data mining characterized by highly skewed misclassification costs certain types of errors become virtually unacceptable . this limits the utility of a classifier to a range in which such constraints can be met . naive bayes , which has proven to be very useful in text mining applications due to high scalability , can be particularly affected . although its 0\/1 loss tends to be small , its misclassifications are often made with apparently high confidence . aside from efforts to better calibrate naive bayes scores , it has been shown that its accuracy depends on document sparsity and feature selection can lead to marked improvement in classification performance . traditionally , sparsity is controlled globally , and the result for any particular document may vary . in this work we examine the merits of local sparsity control for naive bayes in the context of highly asymmetric misclassification costs . in experiments with three benchmark document collections we demonstrate clear advantages of document-level feature selection . in the extreme cost setting , multinomial naive bayes with local sparsity control is able to outperform even some of the recently proposed effective improvements to the naive bayes classifier . there are also indications that local feature selection may be preferable in different cost settings .