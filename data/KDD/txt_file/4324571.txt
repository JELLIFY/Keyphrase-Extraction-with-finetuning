discovering additive structure in black box functions many automated learning procedures lack interpretability , operating effectively as a black box : providing a prediction tool but no explanation of the underlying dynamics that drive it . a common approach to interpretation is to plot the dependence of a learned function on one or two predictors . we present a method that seeks not to display the behavior of a function , but to evaluate the importance of non-additive interactions within any set of variables . should the function be close to a sum of low dimensional components , these components can be viewed and even modeled parametrically . alternatively , the work here provides an indication of where intrinsically high-dimensional behavior takes place . the calculations used in this paper correspond closely with the functional anova decomposition ; a well-developed construction in statistics . in particular , the proposed score of interaction importance measures the loss associated with the projection of the prediction function onto a space of additive models . the algorithm runs in linear time and we present displays of the output as a graphical model of the function for interpretation purposes .