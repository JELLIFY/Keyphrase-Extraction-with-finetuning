efficient kernel feature extraction for massive data sets maximum margin discriminant analysis ( mmda ) was proposed that uses the margin idea for feature extraction . it often outperforms traditional methods like kernel principal component analysis ( kpca ) and kernel fisher discriminant analysis ( kfd ) . however , as in other kernel methods , its time complexity is cubic in the number of training points m , and is thus computationally inefficient on massive data sets . in this paper , we propose an ( 1 + Îµ ) 2-approximation algorithm for obtaining the mmda features by extending the core vector machines . the resultant time complexity is only linear in m , while its space complexity is independent of m. extensive comparisons with the original mmda , kpca , and kfd on a number of large data sets show that the proposed feature extractor can improve classification accuracy , and is also faster than these kernel-based methods by more than an order of magnitude .