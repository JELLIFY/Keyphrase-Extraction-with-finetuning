large linear classification when data can not fit in memory recent advances in linear classification have shown that for applications such as document classification , the training can be extremely efficient . however , most of the existing training methods are designed by assuming that data can be stored in the computer memory . these methods can not be easily applied to data larger than the memory capacity due to the random access to the disk . we propose and analyze a block minimization framework for data larger than the memory size . at each step a block of data is loaded from the disk and handled by certain learning methods . we investigate two implementations of the proposed framework for primal and dual svms , respectively . as data can not fit in memory , many design considerations are very different from those for traditional algorithms . experiments using data sets 20 times larger than the memory demonstrate the effectiveness of the proposed method .