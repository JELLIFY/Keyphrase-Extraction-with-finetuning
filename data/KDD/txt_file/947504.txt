feature selection methods for text classification we consider feature selection for text classification both theoretically and empirically . our main result is an unsupervised feature selection strategy for which we give worst-case theoretical guarantees on the generalization power of the resultant classification function f with respect to the classification function f obtained when keeping all the features . to the best of our knowledge , this is the first feature selection method with such guarantees . in addition , the analysis leads to insights as to when and why this feature selection strategy will perform well in practice . we then use the techtc-100 , 20-newsgroups , and reuters-rcv2 data sets to evaluate empirically the performance of this and two simpler but related feature selection strategies against two commonly-used strategies . our empirical evaluation shows that the strategy with provable performance guarantees performs well in comparison with other commonly-used feature selection strategies . in addition , it performs better on certain datasets under very aggressive feature selection .