unsupervised feature selection for principal components analysis principal components analysis ( pca ) is the predominant linear dimensionality reduction technique , and has been widely applied on datasets in all scientific domains . we consider , both theoretically and empirically , the topic of unsupervised feature selection for pca , by leveraging algorithms for the so-called column subset selection problem ( cssp ) . in words , the cssp seeks the `` best '' subset of exactly k columns from an m x n data matrix a , and has been extensively studied in the numerical linear algebra community . we present a novel two-stage algorithm for the cssp . from a theoretical perspective , for small to moderate values of k , this algorithm significantly improves upon the best previously-existing results ( 24 , 12 ) for the cssp . from an empirical perspective , we evaluate this algorithm as an unsupervised feature selection strategy in three application domains of modern statistical data analysis : finance , document-term data , and genetics . we pay particular attention to how this algorithm may be used to select representative or landmark features from an object-feature matrix in an unsupervised manner . in all three application domains , we are able to identify k landmark features , i.e. , columns of the data matrix , that capture nearly the same amount of information as does the subspace that is spanned by the top k `` eigenfeatures . ''