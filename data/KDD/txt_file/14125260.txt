large-scale matrix factorization with distributed stochastic gradient descent we provide a novel algorithm to approximately factor large matrices with millions of rows , millions of columns , and billions of nonzero elements . our approach rests on stochastic gradient descent ( sgd ) , an iterative stochastic optimization algorithm . we first develop a novel `` stratified '' sgd variant ( ssgd ) that applies to general loss-minimization problems in which the loss function can be expressed as a weighted sum of `` stratum losses . '' we establish sufficient conditions for convergence of ssgd using results from stochastic approximation theory and regenerative process theory . we then specialize ssgd to obtain a new matrix-factorization algorithm , called dsgd , that can be fully distributed and run on web-scale datasets using , e.g. , mapreduce . dsgd can handle a wide variety of matrix factorizations . we describe the practical techniques used to optimize performance in our dsgd implementation . experiments suggest that dsgd converges significantly faster and has better scalability properties than alternative algorithms .