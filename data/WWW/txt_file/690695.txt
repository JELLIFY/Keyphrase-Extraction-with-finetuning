user-centric web crawling search engines are the primary gateways of information access on the web today . behind the scenes , search engines crawl the web to populate a local indexed repository of web pages , used to answer user search queries . in an aggregate sense , the web is very dynamic , causing any repository of web pages to become out of date over time , which in turn causes query answer quality to degrade . given the considerable size , dynamicity , and degree of autonomy of the web as a whole , it is not feasible for a search engine to maintain its repository exactly synchronized with the web . in this paper we study how to schedule web pages for selective ( re ) downloading into a search engine repository . the scheduling objective is to maximize the quality of the user experience for those who query the search engine . we begin with a quantitative characterization of the way in which the discrepancy between the content of the repository and the current content of the live web impacts the quality of the user experience . this characterization leads to a user-centric metric of the quality of a search engine 's local repository . we use this metric to derive a policy for scheduling web page ( re ) downloading that is driven by search engine usage and free of exterior tuning parameters . we then focus on the important subproblem of scheduling refreshing of web pages already present in the repository , and show how to compute the priorities efficiently . we provide extensive empirical comparisons of our user-centric method against prior web page refresh strategies , using real web data . our results demonstrate that our method requires far fewer resources to maintain same search engine quality level for users , leaving substantially more resources available for incorporating new web pages into the search repository . 