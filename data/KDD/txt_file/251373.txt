statistical modeling of large-scale simulation data with the advent of fast computer systems , scientists are now able to generate terabytes of simulation data . unfortunately , the sheer size of these data sets has made efficient exploration of them impossible . to aid scientists in gleaning insight from their simulation data , we have developed an ad-hoc query infrastructure . our system , called aqsim ( short for ad-hoc queries for simulation ) reduces the data storage requirements and query access times in two stages . first , it creates and stores mathematical and statistical models of the data at multiple resolutions . second , it evaluates queries on the models of the data instead of on the entire data set . in this paper , we present two simple but effective statistical modeling techniques for simulation data . our first modeling technique computes the `` true '' ( unbiased ) mean of systematic partitions of the data . it makes no assumptions about the distribution of the data and uses a variant of the root mean square error to evaluate a model . our second statistical modeling technique uses the andersen-darling goodness-of-fit method on systematic partitions of the data . this method evaluates a model by how well it passes the normality test on the data . both of our statistical models effectively answer range queries . at each resolution of the data , we compute the precision of our answer to the user 's query by scaling the one-sided chebyshev inequalities with the original mesh 's topology . we combine precisions at different resolutions by calculating their weighted average . our experimental evaluations on two scientific simulation data sets illustrate the value of using these statistical modeling techniques on multiple resolutions of large simulation data sets .