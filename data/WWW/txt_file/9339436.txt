less talk , more rock : automated organization of community-contributed collections of concert videos we describe a system for synchronization and organization of user-contributed content from live music events . we start with a set of short video clips taken at a single event by multiple contributors , who were using a varied set of capture devices . using audio fingerprints , we synchronize these clips such that overlapping clips can be displayed simultaneously . furthermore , we use the timing and link structure generated by the synchronization algorithm to improve the findability and representation of the event content , including identifying key moments of interest and descriptive text for important captured segments of the show . we also identify the preferred audio track when multiple clips overlap . we thus create a much improved representation of the event that builds on the automatic content match . our work demonstrates important principles in the use of content analysis techniques for social media content on the web , and applies those principles in the domain of live music capture . 