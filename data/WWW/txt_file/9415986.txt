how opinions are received by online communities : a case study on amazon.com helpfulness votes there are many on-line settings in which users publicly express opinions . a number of these offer mechanisms for other users to evaluate these opinions ; a canonical example is amazon.com , where reviews come with annotations like `` 26 of 32 people found the following review helpful . '' opinion evaluation appears in many off-line settings as well , including market research and political campaigns . reasoning about the evaluation of an opinion is fundamentally different from reasoning about the opinion itself : rather than asking , `` what did y think of x ? '' , we are asking , `` what did z think of y 's opinion of x ? '' here we develop a framework for analyzing and modeling opinion evaluation , using a large-scale collection of amazon book reviews as a dataset . we find that the perceived helpfulness of a review depends not just on its content but also but also in subtle ways on how the expressed evaluation relates to other evaluations of the same product . as part of our approach , we develop novel methods that take advantage of the phenomenon of review `` plagiarism '' to control for the effects of text in opinion evaluation , and we provide a simple and natural mathematical model consistent with our findings . our analysis also allows us to distinguish among the predictions of competing theories from sociology and social psychology , and to discover unexpected differences in the collective opinion-evaluation behavior of user populations from ifferent countries . 