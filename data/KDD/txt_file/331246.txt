automatic multimedia cross-modal correlation discovery given an image ( or video clip , or audio song ) , how do we automatically assign keywords to it ? the general problem is to find correlations across the media in a collection of multimedia objects like video clips , with colors , and\/or motion , and\/or audio , and\/or text scripts . we propose a novel , graph-based approach , `` mmg '' , to discover such cross-modal correlations . our `` mmg '' method requires no tuning , no clustering , no user-determined constants ; it can be applied to any multimedia collection , as long as we have a similarity function for each medium ; and it scales linearly with the database size . we report auto-captioning experiments on the `` standard '' corel image database of 680 mb , where it outperforms domain specific , fine-tuned methods by up to 10 percentage points in captioning accuracy ( 50 % relative improvement ) .