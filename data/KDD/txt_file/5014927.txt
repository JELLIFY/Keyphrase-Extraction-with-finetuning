learning the kernel matrix in discriminant analysis via quadratically constrained quadratic programming the kernel function plays a central role in kernel methods . in this paper , we consider the automated learning of the kernel matrix over a convex combination of pre-specified kernel matrices in regularized kernel discriminant analysis ( rkda ) , which performs lineardiscriminant analysis in the feature space via the kernel trick . previous studies have shown that this kernel learning problem can be formulated as a semidefinite program ( sdp ) , which is however computationally expensive , even with the recent advances in interior point methods . based on the equivalence relationship between rkda and least square problems in the binary-class case , we propose a quadratically constrained quadratic programming ( qcqp ) formulation for the kernel learning problem , which can be solved more efficiently than sdp . while most existing work on kernel learning deal with binary-class problems only , we show that our qcqp formulation can be extended naturally to the multi-class case . experimental results on both binary-class and multi-class benchmarkdata sets show the efficacy of the proposed qcqp formulations .