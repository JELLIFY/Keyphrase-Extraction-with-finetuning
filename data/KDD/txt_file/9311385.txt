learning subspace kernels for classification kernel methods have been applied successfully in many data mining tasks . subspace kernel learning was recently proposed to discover an effective low-dimensional subspace of a kernel feature space for improved classification . in this paper , we propose to construct a subspace kernel using the hilbert-schmidt independence criterion ( hsic ) . we show that the optimal subspace kernel can be obtained efficiently by solving an eigenvalue problem . one limitation of the existing subspace kernel learning formulations is that the kernel learning and classification are independent and the subspace kernel may not be optimally adapted for classification . to overcome this limitation , we propose a joint optimization framework , in which we learn the subspace kernel and subsequent classifiers simultaneously . in addition , we propose a novel learning formulation that extracts an uncorrelated subspace kernel to reduce the redundant information in a subspace kernel . following the idea from multiple kernel learning , we extend the proposed formulations to the case when multiple kernels are available and need to be combined . we show that the integration of subspace kernels can be formulated as a semidefinite program ( sdp ) which is computationally expensive . to improve the efficiency of the sdp formulation , we propose an equivalent semi-infinite linear program ( silp ) formulation which can be solved efficiently by the column generation technique . experimental results on a collection of benchmark data sets demonstrate the effectiveness of the proposed algorithms .