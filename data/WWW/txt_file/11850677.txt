learning to re-rank : query-dependent image re-ranking using click data our objective is to improve the performance of keyword based image search engines by re-ranking their original results . to this end , we address three limitations of existing search engines in this paper . first , there is no straight-forward , fully automated way of going from textual queries to visual features . image search engines therefore primarily rely on static and textual features for ranking . visual features are mainly used for secondary tasks such as finding similar images . second , image rankers are trained on query-image pairs labeled with relevance judgments determined by human experts . such labels are well known to be noisy due to various factors including ambiguous queries , unknown user intent and subjectivity in human judgments . this leads to learning a sub-optimal ranker . finally , a static ranker is typically built to handle disparate user queries . the ranker is therefore unable to adapt its parameters to suit the query at hand which again leads to sub-optimal results . we demonstrate that all of these problems can be mitigated by employing a re-ranking algorithm that leverages aggregate user click data . we hypothesize that images clicked in response to a query are mostly relevant to the query . we therefore re-rank the original search results so as to promote images that are likely to be clicked to the top of the ranked list . our re-ranking algorithm employs gaussian process regression to predict the normalized click count for each image , and combines it with the original ranking score . our approach is shown to significantly boost the performance of the bing image search engine on a wide range of tail queries . 