fast coordinate descent methods with variable selection for non-negative matrix factorization nonnegative matrix factorization ( nmf ) is an effective dimension reduction method for non-negative dyadic data , and has proven to be useful in many areas , such as text mining , bioinformatics and image processing . nmf is usually formulated as a constrained non-convex optimization problem , and many algorithms have been developed for solving it . recently , a coordinate descent method , called fasthals , has been proposed to solve least squares nmf and is regarded as one of the state-of-the-art techniques for the problem . in this paper , we first show that fasthals has an inefficiency in that it uses a cyclic coordinate descent scheme and thus , performs unneeded descent steps on unimportant variables . we then present a variable selection scheme that uses the gradient of the objective function to arrive at a new coordinate descent method . our new method is considerably faster in practice and we show that it has theoretical convergence guarantees . moreover when the solution is sparse , as is often the case in real applications , our new method benefits by selecting important variables to update more often , thus resulting in higher speed . as an example , on a text dataset rcv1 , our method is 7 times faster than fasthals , and more than 15 times faster when the sparsity is increased by adding an l1 penalty . we also develop new coordinate descent methods when error in nmf is measured by kl-divergence by applying the newton method to solve the one-variable sub-problems . experiments indicate that our algorithm for minimizing the kl-divergence is faster than the lee & seung multiplicative rule by a factor of 10 on the cbcl image dataset .