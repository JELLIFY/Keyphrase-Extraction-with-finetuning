efficient methods for topic model inference on streaming document collections topic models provide a powerful tool for analyzing large text collections by representing high dimensional data in a low dimensional subspace . fitting a topic model given a set of training documents requires approximate inference techniques that are computationally expensive . with today 's large-scale , constantly expanding document collections , it is useful to be able to infer topic distributions for new documents without retraining the model . in this paper , we empirically evaluate the performance of several methods for topic inference in previously unseen documents , including methods based on gibbs sampling , variational inference , and a new method inspired by text classification . the classification-based inference method produces results similar to iterative inference methods , but requires only a single matrix multiplication . in addition to these inference methods , we present sparselda , an algorithm and data structure for evaluating gibbs sampling distributions . empirical results indicate that sparselda can be approximately 20 times faster than traditional lda and provide twice the speedup of previously published fast sampling methods , while also using substantially less memory .