a large-scale study of robots . txt search engines largely rely on web robots to collect information from the web . due to the unregulated open-access nature of the web , robot activities are extremely diverse . such crawling activities can be regulated from the server side by deploying the robots exclusion protocol in a file called robots . txt . although it is not an enforcement standard , ethical robots ( and many commercial ) will follow the rules specified in robots . txt . with our focused crawler , we investigate 7,593 websites from education , government , news , and business domains . five crawls have been conducted in succession to study the temporal changes . through statistical analysis of the data , we present a survey of the usage of web robots rules at the web scale . the results also show that the usage of robots . txt has increased over time . 