robust boosting and its relation to bagging several authors have suggested viewing boosting as a gradient descent search for a good fit in function space . at each iteration observations are re-weighted using the gradient of the underlying loss function . we present an approach of weight decay for observation weights which is equivalent to `` robustifying '' the underlying loss function . at the extreme end of decay this approach converges to bagging , which can be viewed as boosting with a linear underlying loss function . we illustrate the practical usefulness of weight decay for improving prediction performance and present an equivalence between one form of weight decay and `` huberizing '' -- a statistical method for making loss functions more robust .