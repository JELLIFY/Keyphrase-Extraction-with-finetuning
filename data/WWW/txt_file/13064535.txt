two-stream indexing for spoken web search this paper presents two-stream processing of audio to index the audio content for spoken web search . the first stream indexes the meta-data associated with a particular audio document . the meta-data is usually very sparse , but accurate . this therefore results in a high-precision , low-recall index . the second stream uses a novel language-independent speech recognition to generate text to be indexed . owing to the multiple languages and the noise in user generated content on the spoken web , the speech recognition accuracy of such systems is not high , thus they result in a low-precision , high-recall index . the paper attempts to use these two complementary streams to generate a combined index to increase the precision-recall performance in audio content search . the problem of audio content search is motivated by the real world implication of the web in developing regions , where due to literacy and affordability issues , people use spoken web which consists of interconnected voicesites , which have content in audio . the experiments are based on more than 20,000 audio documents spanning over seven live voicesites and four different languages . the results suggest significant improvement over a meta-data-only or a speech-recognitiononly system , thus justifying the two-stream processing approach . audio content search is a growing problem area and this paper wishes to be a first step to solving this at a large scale , across languages , in a web context . 