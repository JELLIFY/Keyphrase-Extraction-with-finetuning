compressed web indexes web search engines use indexes to efficiently retrieve pages containing specified query terms , as well as pages linking to specified pages . the problem of compressed indexes that permit such fast retrieval has a long history . we consider the problem : assuming that the terms in ( or links to ) a page are generated from a probability distribution , how well compactly can we build such indexes that allow fast retrieval ? of particular interest is the case when the probability distribution is zipfian ( or a similar power law ) , since these are the distributions that arise on the web . we obtain sharp bounds on the space requirement of boolean indexes for text documents that follow zipf 's law . in the process we develop a general technique that applies to any probability distribution , not necessarily a power law ; this is the first analysis of compression in indexes under arbitrary distributions . our bounds lead to quantitative versions of rules of thumb that are folklore in indexing . our experiments on several document collections show that the distribution of terms appears to follow a double-pareto law rather than zipf 's law . despite widely varying sets of documents , the index sizes observed in the experiments conform well to our theoretical predictions . 